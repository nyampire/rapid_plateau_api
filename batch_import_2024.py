#!/usr/bin/env python3
"""
2024å¹´åº¦Plateauéƒ½å¸‚ãƒ‡ãƒ¼ã‚¿ ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å„éƒ½å¸‚ã‚’é †ç•ªã«å‡¦ç†:
  1. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆplateau_downloader.pyï¼‰
  2. PostGISã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆplateau_importer2postgis.pyï¼‰
  3. ZIPãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç¯€ç´„ï¼‰

ä½¿ç”¨ä¾‹:
  # å…¨2024å¹´åº¦éƒ½å¸‚ã‚’å‡¦ç†
  python batch_import_2024.py --postgres-url "postgresql://user:pass@localhost/db"

  # ç‰¹å®šã®éƒ½å¸‚ã ã‘å‡¦ç†
  python batch_import_2024.py --postgres-url "..." --citycodes 21211 16211 33423

  # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¤ãƒ³ãƒãƒ¼ãƒˆã›ãšä¸€è¦§è¡¨ç¤ºã®ã¿ï¼‰
  python batch_import_2024.py --dry-run

  # æ—¢ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿ã®éƒ½å¸‚ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šãã‹ã‚‰
  python batch_import_2024.py --postgres-url "..." --skip-imported
"""

import subprocess
import sys
import os
import time
import json
import logging
import shutil
import argparse
from pathlib import Path
from datetime import datetime

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_import_2024.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# 2024å¹´åº¦éƒ½å¸‚ã‚³ãƒ¼ãƒ‰ä¸€è¦§ï¼ˆsurveyor.mydns.jpã®folderåã«"2024"ã‚’å«ã‚€éƒ½å¸‚ï¼‰
# æ—¢ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿ã®éƒ½å¸‚ï¼ˆ12221å…«åƒä»£å¸‚ã€33423æ—©å³¶ç”ºã€16211å°„æ°´å¸‚ï¼‰ã¯å«ã‚ãªã„
CITIES_2024 = [
    "03201",  # ç››å²¡å¸‚
    "03202",  # å®®å¤å¸‚
    "04100",  # ä»™å°å¸‚
    "05204",  # å¤§é¤¨å¸‚
    "07201",  # ç¦å³¶å¸‚
    "11100",  # ã•ã„ãŸã¾å¸‚
    "11202",  # ç†Šè°·å¸‚
    "11203",  # å·å£å¸‚
    "11208",  # æ‰€æ²¢å¸‚
    "11217",  # é´»å·£å¸‚
    "11223",  # è•¨å¸‚
    "11228",  # å¿—æœ¨å¸‚
    "11230",  # æ–°åº§å¸‚
    "11235",  # å¯Œå£«è¦‹å¸‚
    "11237",  # ä¸‰éƒ·å¸‚
    "11240",  # å¹¸æ‰‹å¸‚
    "11241",  # é¶´ãƒ¶å³¶å¸‚
    "11301",  # ä¼Šå¥ˆç”º
    "11324",  # ä¸‰èŠ³ç”º
    "11385",  # ä¸Šé‡Œç”º
    "12100",  # åƒè‘‰å¸‚
    "12206",  # æœ¨æ›´æ´¥å¸‚
    "13106",  # å°æ±åŒº
    "13107",  # å¢¨ç”°åŒº
    "13115",  # æ‰ä¸¦åŒº
    "13201",  # å…«ç‹å­å¸‚
    "13202",  # ç«‹å·å¸‚
    "13203",  # æ­¦è”µé‡å¸‚
    "13204",  # ä¸‰é·¹å¸‚
    "13205",  # é’æ¢…å¸‚
    "13206",  # åºœä¸­å¸‚
    "13207",  # æ˜­å³¶å¸‚
    "13209",  # ç”ºç”°å¸‚
    "13210",  # å°é‡‘äº•å¸‚
    "13211",  # å°å¹³å¸‚
    "13212",  # æ—¥é‡å¸‚
    "13213",  # æ±æ‘å±±å¸‚
    "13214",  # å›½åˆ†å¯ºå¸‚
    "13215",  # å›½ç«‹å¸‚
    "13218",  # ç¦ç”Ÿå¸‚
    "13219",  # ç‹›æ±Ÿå¸‚
    "13220",  # æ±å¤§å’Œå¸‚
    "13221",  # æ¸…ç€¬å¸‚
    "13223",  # æ­¦è”µæ‘å±±å¸‚
    "13224",  # å¤šæ‘©å¸‚
    "13225",  # ç¨²åŸå¸‚
    "13228",  # ã‚ãã‚‹é‡å¸‚
    "13229",  # è¥¿æ±äº¬å¸‚
    "13303",  # ç‘ç©‚ç”º
    "13308",  # å¥¥å¤šæ‘©ç”º
    "13361",  # å¤§å³¶ç”º
    "13362",  # åˆ©å³¶æ‘
    "13363",  # æ–°å³¶æ‘
    "13364",  # ç¥æ´¥å³¶æ‘
    "13381",  # ä¸‰å®…æ‘
    "13401",  # å…«ä¸ˆç”º
    "13402",  # é’ãƒ¶å³¶æ‘
    "14100",  # æ¨ªæµœå¸‚
    "14150",  # ç›¸æ¨¡åŸå¸‚
    "14204",  # éŒå€‰å¸‚
    "15202",  # é•·å²¡å¸‚
    "16202",  # é«˜å²¡å¸‚
    "17201",  # é‡‘æ²¢å¸‚
    "17206",  # åŠ è³€å¸‚
    "20220",  # å®‰æ›‡é‡å¸‚
    "21201",  # å²é˜œå¸‚
    "21202",  # å¤§å£å¸‚
    "21211",  # ç¾æ¿ƒåŠ èŒ‚å¸‚
    "22100",  # é™å²¡å¸‚
    "22130",  # æµœæ¾å¸‚
    "22203",  # æ²¼æ´¥å¸‚
    "22205",  # ç†±æµ·å¸‚
    "22206",  # ä¸‰å³¶å¸‚
    "22207",  # å¯Œå£«å®®å¸‚
    "22208",  # ä¼Šæ±å¸‚
    "22209",  # å³¶ç”°å¸‚
    "22210",  # å¯Œå£«å¸‚
    "22211",  # ç£ç”°å¸‚
    "22212",  # ç„¼æ´¥å¸‚
    "22213",  # æ›å·å¸‚
    "22214",  # è—¤æå¸‚
    "22215",  # å¾¡æ®¿å ´å¸‚
    "22216",  # è¢‹äº•å¸‚
    "22219",  # ä¸‹ç”°å¸‚
    "22220",  # è£¾é‡å¸‚
    "22221",  # æ¹–è¥¿å¸‚
    "22222",  # ä¼Šè±†å¸‚
    "22223",  # å¾¡å‰å´å¸‚
    "22224",  # èŠå·å¸‚
    "22225",  # ä¼Šè±†ã®å›½å¸‚
    "22226",  # ç‰§ä¹‹åŸå¸‚
    "22301",  # æ±ä¼Šè±†ç”º
    "22302",  # æ²³æ´¥ç”º
    "22304",  # å—ä¼Šè±†ç”º
    "22305",  # æ¾å´ç”º
    "22306",  # è¥¿ä¼Šè±†ç”º
    "22325",  # å‡½å—ç”º
    "22341",  # æ¸…æ°´ç”º
    "22342",  # é•·æ³‰ç”º
    "22344",  # å°å±±ç”º
    "22424",  # å‰ç”°ç”º
    "22429",  # å·æ ¹æœ¬ç”º
    "22461",  # æ£®ç”º
    "23100",  # åå¤å±‹å¸‚
    "23201",  # è±Šæ©‹å¸‚
    "23206",  # æ˜¥æ—¥äº•å¸‚
    "23211",  # è±Šç”°å¸‚
    "23230",  # æ—¥é€²å¸‚
    "24203",  # ä¼Šå‹¢å¸‚
    "26100",  # äº¬éƒ½å¸‚
    "27100",  # å¤§é˜ªå¸‚
    "27140",  # å ºå¸‚
    "27202",  # å²¸å’Œç”°å¸‚
    "27216",  # æ²³å†…é•·é‡å¸‚
    "27227",  # æ±å¤§é˜ªå¸‚
    "28201",  # å§«è·¯å¸‚
    "28215",  # ä¸‰æœ¨å¸‚
    "28229",  # ãŸã¤ã®å¸‚
    "30201",  # å’Œæ­Œå±±å¸‚
    "30406",  # ã™ã•ã¿ç”º
    "31202",  # ç±³å­å¸‚
    "32204",  # ç›Šç”°å¸‚
    "32528",  # éš å²ã®å³¶ç”º
    "33202",  # å€‰æ•·å¸‚
    "33211",  # å‚™å‰å¸‚
    "34100",  # åºƒå³¶å¸‚
    "34304",  # æµ·ç”°ç”º
    "36201",  # å¾³å³¶å¸‚
    "37206",  # ã•ã¬ãå¸‚
    "39386",  # ã„ã®ç”º
    "40130",  # ç¦å²¡å¸‚
    "40202",  # å¤§ç‰Ÿç”°å¸‚
    "40223",  # å¤è³€å¸‚
    "41203",  # é³¥æ –å¸‚
    "42208",  # æ¾æµ¦å¸‚
    "42323",  # æ³¢ä½è¦‹ç”º
    "43206",  # ç‰åå¸‚
]

# æ—¢ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿ã®éƒ½å¸‚ï¼ˆã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ï¼‰
ALREADY_IMPORTED = {"12221", "33423", "16211"}

# å¤§è¦æ¨¡éƒ½å¸‚ï¼ˆå¾Œå›ã—ã«ã™ã‚‹ï¼‰
LARGE_CITIES = {
    "04100",  # ä»™å°å¸‚
    "11100",  # ã•ã„ãŸã¾å¸‚
    "13201",  # å…«ç‹å­å¸‚
    "13209",  # ç”ºç”°å¸‚
    "14100",  # æ¨ªæµœå¸‚
    "14150",  # ç›¸æ¨¡åŸå¸‚
    "22100",  # é™å²¡å¸‚
    "22130",  # æµœæ¾å¸‚
    "23100",  # åå¤å±‹å¸‚
    "23211",  # è±Šç”°å¸‚
    "26100",  # äº¬éƒ½å¸‚
    "27100",  # å¤§é˜ªå¸‚
    "27140",  # å ºå¸‚
    "34100",  # åºƒå³¶å¸‚
    "40130",  # ç¦å²¡å¸‚
}


def get_done_dir(base_dir: Path) -> Path:
    """å®Œäº†è¨˜éŒ²ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—"""
    done_dir = base_dir / ".done"
    done_dir.mkdir(parents=True, exist_ok=True)
    return done_dir


def mark_city_done(base_dir: Path, citycode: str):
    """éƒ½å¸‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ã‚’è¨˜éŒ²"""
    done_file = get_done_dir(base_dir) / f"{citycode}.done"
    done_file.write_text(datetime.now().isoformat())
    logger.info(f"ğŸ“ [{citycode}] å®Œäº†è¨˜éŒ²: {done_file}")


def get_done_citycodes(base_dir: Path) -> set:
    """å®Œäº†è¨˜éŒ²ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿éƒ½å¸‚ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—"""
    done_dir = get_done_dir(base_dir)
    done_codes = set()
    for f in done_dir.glob("*.done"):
        done_codes.add(f.stem)
    return done_codes


def get_imported_citycodes(postgres_url: str) -> set:
    """DBã‹ã‚‰æ—¢ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿ã®éƒ½å¸‚ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    try:
        import psycopg2
        conn = psycopg2.connect(postgres_url)
        cursor = conn.cursor()
        cursor.execute("""
            SELECT DISTINCT source_dataset
            FROM plateau_buildings
            WHERE source_dataset IS NOT NULL
        """)
        datasets = cursor.fetchall()
        conn.close()

        # source_datasetã‹ã‚‰citycodeã‚’æŠ½å‡ºï¼ˆä¾‹: "plateau_03201_59413067_bldg_6697_op.osm" â†’ "03201"ï¼‰
        imported = set()
        for (dataset,) in datasets:
            if dataset:
                import re
                match = re.search(r'(\d{5})', dataset)
                if match:
                    imported.add(match.group(1))
        return imported
    except Exception as e:
        logger.warning(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿éƒ½å¸‚ã®ç¢ºèªã«å¤±æ•—: {e}")
        return set()


def cleanup_orphan_nodes(postgres_url: str):
    """å­¤å…ãƒãƒ¼ãƒ‰ï¼ˆå¯¾å¿œã™ã‚‹å»ºç‰©ãŒãªã„ãƒãƒ¼ãƒ‰ï¼‰ã‚’å‰Šé™¤"""
    try:
        import psycopg2 as pg2
        conn = pg2.connect(postgres_url)
        cursor = conn.cursor()

        # å­¤å…ãƒãƒ¼ãƒ‰æ•°ã‚’ç¢ºèª
        cursor.execute("""
            SELECT COUNT(*) FROM plateau_building_nodes n
            WHERE NOT EXISTS (
                SELECT 1 FROM plateau_buildings b WHERE b.id = n.building_id
            )
        """)
        orphan_count = cursor.fetchone()[0]

        if orphan_count > 0:
            logger.info(f"ğŸ§¹ å­¤å…ãƒãƒ¼ãƒ‰æ¤œå‡º: {orphan_count}ä»¶ â€” å‰Šé™¤ä¸­...")
            cursor.execute("""
                DELETE FROM plateau_building_nodes n
                WHERE NOT EXISTS (
                    SELECT 1 FROM plateau_buildings b WHERE b.id = n.building_id
                )
            """)
            conn.commit()
            logger.info(f"âœ… å­¤å…ãƒãƒ¼ãƒ‰ {orphan_count}ä»¶ã‚’å‰Šé™¤")
        else:
            logger.info(f"âœ… å­¤å…ãƒãƒ¼ãƒ‰ãªã—")

        conn.close()
    except Exception as e:
        logger.warning(f"âš ï¸ å­¤å…ãƒãƒ¼ãƒ‰ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")


def process_city(citycode: str, base_dir: Path, postgres_url: str, python_cmd: str) -> dict:
    """1éƒ½å¸‚ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰â†’ã‚¤ãƒ³ãƒãƒ¼ãƒˆâ†’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    data_dir = base_dir / citycode
    result = {
        "citycode": citycode,
        "download_ok": False,
        "import_ok": False,
        "cleanup_ok": False,
        "error": None,
        "start_time": datetime.now().isoformat(),
    }

    try:
        # Phase 0: ãƒ‡ã‚£ã‚¹ã‚¯æ®‹é‡ãƒã‚§ãƒƒã‚¯ & å­¤å…ãƒãƒ¼ãƒ‰ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        disk_usage = shutil.disk_usage(str(base_dir))
        free_gb = disk_usage.free / (1024**3)
        logger.info(f"ğŸ’¿ ãƒ‡ã‚£ã‚¹ã‚¯æ®‹é‡: {free_gb:.1f} GB")

        if free_gb < 5.0:
            logger.error(f"âŒ [{citycode}] ãƒ‡ã‚£ã‚¹ã‚¯æ®‹é‡ä¸è¶³ ({free_gb:.1f} GB < 5 GB) â€” ä¸­æ–­")
            result["error"] = f"disk_full ({free_gb:.1f}GB free)"
            return result

        cleanup_orphan_nodes(postgres_url)

        # Phase 1: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        logger.info(f"ğŸ“¥ [{citycode}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
        dl_cmd = [
            python_cmd, "plateau_downloader.py",
            "--citycode", citycode,
            "--output-dir", str(data_dir)
        ]
        dl_result = subprocess.run(dl_cmd, text=True, timeout=1800)

        if dl_result.returncode != 0:
            logger.error(f"âŒ [{citycode}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
            result["error"] = "download_failed"
            return result

        # ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ç¢ºèª
        zip_files = list(data_dir.glob("*.zip"))
        if not zip_files:
            logger.warning(f"âš ï¸ [{citycode}] ZIPãƒ•ã‚¡ã‚¤ãƒ«ãªã— â€” ã‚¹ã‚­ãƒƒãƒ—")
            result["error"] = "no_zip_files"
            return result

        result["download_ok"] = True
        logger.info(f"âœ… [{citycode}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(zip_files)}ãƒ•ã‚¡ã‚¤ãƒ«")

        # Phase 2: ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        logger.info(f"ğŸ“¦ [{citycode}] ã‚¤ãƒ³ãƒãƒ¼ãƒˆé–‹å§‹...")
        import_cmd = [
            python_cmd, "plateau_importer2postgis.py",
            "--data-dir", str(data_dir),
            "--postgres-url", postgres_url,
            "--citycode", citycode
        ]
        import_result = subprocess.run(import_cmd, text=True, timeout=3600)

        if import_result.returncode != 0:
            logger.error(f"âŒ [{citycode}] ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—")
            result["error"] = "import_failed"
            return result

        # Phase 2.5: DBã«ãƒ‡ãƒ¼ã‚¿ãŒå®Ÿéš›ã«å…¥ã£ãŸã‹æ¤œè¨¼
        try:
            import psycopg2 as pg2
            verify_conn = pg2.connect(postgres_url)
            verify_cur = verify_conn.cursor()
            verify_cur.execute(
                "SELECT COUNT(*) FROM plateau_buildings WHERE source_dataset LIKE %s",
                (f"%{citycode}%",)
            )
            db_count = verify_cur.fetchone()[0]
            verify_conn.close()

            if db_count == 0:
                logger.error(f"âŒ [{citycode}] DBæ¤œè¨¼å¤±æ•—: å»ºç‰©ãƒ‡ãƒ¼ã‚¿0ä»¶")
                result["error"] = "import_no_data_in_db"
                return result

            logger.info(f"âœ… [{citycode}] DBæ¤œè¨¼OK: {db_count}ä»¶ã®å»ºç‰©")
        except Exception as e:
            logger.warning(f"âš ï¸ [{citycode}] DBæ¤œè¨¼ã‚¹ã‚­ãƒƒãƒ—: {e}")

        result["import_ok"] = True
        logger.info(f"âœ… [{citycode}] ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")

        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ã‚’è¨˜éŒ²
        mark_city_done(base_dir, citycode)

    except subprocess.TimeoutExpired:
        result["error"] = "timeout"
        logger.error(f"âŒ [{citycode}] ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
    except Exception as e:
        result["error"] = str(e)
        logger.error(f"âŒ [{citycode}] ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        # ZIPãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆæˆåŠŸãƒ»å¤±æ•—ã‚’å•ã‚ãšå¿…ãšå®Ÿè¡Œï¼‰
        if data_dir.exists():
            logger.info(f"ğŸ—‘ï¸ [{citycode}] ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—...")
            try:
                shutil.rmtree(data_dir)
                result["cleanup_ok"] = True
                logger.info(f"âœ… [{citycode}] ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
            except Exception as e:
                logger.warning(f"âš ï¸ [{citycode}] ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")

    result["end_time"] = datetime.now().isoformat()
    return result


def main():
    parser = argparse.ArgumentParser(description='2024å¹´åº¦Plateauéƒ½å¸‚ ä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ')
    parser.add_argument('--postgres-url', required=False,
                        help='PostgreSQLæ¥ç¶šURLï¼ˆæœªæŒ‡å®šæ™‚ã¯DATABASE_URLç’°å¢ƒå¤‰æ•°ï¼‰')
    parser.add_argument('--citycodes', nargs='+', help='å‡¦ç†ã™ã‚‹éƒ½å¸‚ã‚³ãƒ¼ãƒ‰ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯å…¨2024å¹´åº¦éƒ½å¸‚ï¼‰')
    parser.add_argument('--dry-run', action='store_true', help='ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆä¸€è¦§è¡¨ç¤ºã®ã¿ï¼‰')
    parser.add_argument('--skip-imported', action='store_true', help='ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿éƒ½å¸‚ã‚’ã‚¹ã‚­ãƒƒãƒ—')
    parser.add_argument('--small-first', action='store_true', default=True,
                        help='å°è¦æ¨¡éƒ½å¸‚ã‚’å…ˆã«å‡¦ç†ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰')
    parser.add_argument('--base-dir', default='./plateau_data',
                        help='ä¸€æ™‚ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ./plateau_dataï¼‰')
    parser.add_argument('--city-interval', type=int, default=10,
                        help='éƒ½å¸‚é–“ã®å¾…æ©Ÿç§’æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰')

    args = parser.parse_args()

    # PostgreSQL URL
    postgres_url = args.postgres_url or os.environ.get('DATABASE_URL')
    if not postgres_url and not args.dry_run:
        logger.error("âŒ --postgres-url ã¾ãŸã¯ DATABASE_URL ç’°å¢ƒå¤‰æ•°ãŒå¿…è¦ã§ã™")
        sys.exit(1)

    # Pythonå®Ÿè¡Œãƒ‘ã‚¹
    python_cmd = sys.executable

    # å¯¾è±¡éƒ½å¸‚ã®æ±ºå®š
    if args.citycodes:
        target_cities = args.citycodes
    else:
        target_cities = list(CITIES_2024)

    # æ—¢ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿ã®éƒ½å¸‚ã‚’é™¤å¤–
    base_dir = Path(args.base_dir)
    skip_set = set(ALREADY_IMPORTED)
    if args.skip_imported:
        # .doneãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å®Œäº†æ¸ˆã¿éƒ½å¸‚ã‚’å–å¾—ï¼ˆç¢ºå®Ÿï¼‰
        done_codes = get_done_citycodes(base_dir)
        skip_set = skip_set | done_codes
        logger.info(f"ğŸ“Š å®Œäº†è¨˜éŒ²æ¸ˆã¿éƒ½å¸‚: {len(done_codes)}ä»¶")

        # DBã‹ã‚‰ã‚‚å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if postgres_url:
            db_imported = get_imported_citycodes(postgres_url)
            new_from_db = db_imported - skip_set
            if new_from_db:
                logger.info(f"ğŸ“Š DBå†…è¿½åŠ æ¤œå‡ºï¼ˆ.doneãªã—ï¼‰: {len(new_from_db)}ä»¶ {new_from_db}")
                logger.info(f"   âš ï¸ ã“ã‚Œã‚‰ã¯ä¸å®Œå…¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å¯èƒ½æ€§ã‚ã‚Šã€‚å†å‡¦ç†ã—ã¾ã™ã€‚")
                # .doneãŒãªã„éƒ½å¸‚ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ãªã„ï¼ˆä¸å®Œå…¨ã®å¯èƒ½æ€§ï¼‰

    target_cities = [c for c in target_cities if c not in skip_set]

    # å¤§è¦æ¨¡éƒ½å¸‚ã‚’å¾Œå›ã—ã«ã™ã‚‹
    if args.small_first:
        small_cities = [c for c in target_cities if c not in LARGE_CITIES]
        large_cities = [c for c in target_cities if c in LARGE_CITIES]
        target_cities = small_cities + large_cities
        logger.info(f"ğŸ“Š å‡¦ç†é †åº: å°ã€œä¸­è¦æ¨¡ {len(small_cities)}éƒ½å¸‚ â†’ å¤§è¦æ¨¡ {len(large_cities)}éƒ½å¸‚")

    logger.info(f"ğŸ“Š å¯¾è±¡éƒ½å¸‚: {len(target_cities)}ä»¶")
    logger.info(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {len(skip_set)}ä»¶ ({skip_set})")

    if args.dry_run:
        print(f"\n=== ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³: {len(target_cities)}éƒ½å¸‚ ===")
        for i, code in enumerate(target_cities, 1):
            size_label = "ğŸ™ï¸ å¤§è¦æ¨¡" if code in LARGE_CITIES else "ğŸ˜ï¸ å°ã€œä¸­"
            print(f"  {i:3d}. {code} {size_label}")
        print(f"\nã‚¹ã‚­ãƒƒãƒ—: {skip_set}")
        return

    # å‡¦ç†é–‹å§‹
    base_dir.mkdir(parents=True, exist_ok=True)

    results = []
    success_count = 0
    fail_count = 0
    total = len(target_cities)

    logger.info("=" * 60)
    logger.info(f"ğŸš€ ä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆé–‹å§‹: {total}éƒ½å¸‚")
    logger.info("=" * 60)

    for i, citycode in enumerate(target_cities, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ™ï¸ [{i}/{total}] éƒ½å¸‚ã‚³ãƒ¼ãƒ‰: {citycode}")
        logger.info(f"{'='*60}")

        try:
            result = process_city(citycode, base_dir, postgres_url, python_cmd)
            results.append(result)

            if result["import_ok"]:
                success_count += 1
                logger.info(f"âœ… [{i}/{total}] {citycode} å®Œäº† (æˆåŠŸ: {success_count}, å¤±æ•—: {fail_count})")
            else:
                fail_count += 1
                logger.warning(f"âŒ [{i}/{total}] {citycode} å¤±æ•—: {result.get('error', 'unknown')}")

            # éƒ½å¸‚é–“ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«
            if i < total:
                logger.info(f"â±ï¸ {args.city_interval}ç§’å¾…æ©Ÿ...")
                time.sleep(args.city_interval)

        except KeyboardInterrupt:
            logger.warning(f"\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸­æ–­ ({i}/{total}å‡¦ç†æ¸ˆã¿)")
            break

    # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ")
    logger.info("=" * 60)
    logger.info(f"   æˆåŠŸ: {success_count}/{total}")
    logger.info(f"   å¤±æ•—: {fail_count}/{total}")

    if fail_count > 0:
        logger.warning("   å¤±æ•—éƒ½å¸‚:")
        for r in results:
            if not r["import_ok"]:
                logger.warning(f"     {r['citycode']}: {r.get('error', 'unknown')}")

    # ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONã«ä¿å­˜
    report_file = f"batch_import_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump({
            "total": total,
            "success": success_count,
            "failed": fail_count,
            "results": results
        }, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")


if __name__ == "__main__":
    main()
