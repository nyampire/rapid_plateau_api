#!/usr/bin/env python3
"""
Plateauå»ºç‰©ãƒ‡ãƒ¼ã‚¿ PostGISã‚¤ãƒ³ãƒãƒ¼ã‚¿ãƒ¼
ãƒ­ãƒ¼ã‚«ãƒ«ã®zipãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰PostgreSQLã«å®‰å…¨ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

å‰ææ¡ä»¶:
- Plateauå»ºç‰©ãƒ‡ãƒ¼ã‚¿ã®zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—æ¸ˆã¿
- PostgreSQL/PostGISãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒæº–å‚™æ¸ˆã¿
"""

import os
import sys
import zipfile
import xml.etree.ElementTree as ET
from pathlib import Path
import psycopg2
from psycopg2.extras import execute_values
import logging
from typing import List, Dict, Tuple, Set
import time
import hashlib
import re
from collections import defaultdict

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('plateau_importer2postgis.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PlateauImporter2PostGIS:
    def __init__(self,
                 data_dir="./plateau_data",
                 postgres_url="postgresql://osmfj_user:secure_plateau_password@localhost:5432/osmfj_plateau",
                 coord_bounds=None,
                 citycode=None):
        """
        Args:
            data_dir: zipãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            postgres_url: PostgreSQLæ¥ç¶šURL
            coord_bounds: åº§æ¨™ç¯„å›²ãƒã‚§ãƒƒã‚¯ç”¨ (min_lat, max_lat, min_lon, max_lon)ã€‚Noneã§ç„¡åŠ¹åŒ–
            citycode: å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰ (ä¾‹: "31202")ã€‚Noneã®å ´åˆã¯data_dirã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰æ¨å®š
        """
        self.data_dir = Path(data_dir)
        self.postgres_url = postgres_url
        self.coord_bounds = coord_bounds

        # å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰ã®æ±ºå®š
        if citycode:
            self.citycode = citycode
        else:
            # data_dirã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰æ¨å®š (ä¾‹: ./plateau_data/31202 â†’ "31202")
            dirname = self.data_dir.name
            match = re.match(r'^(\d{5})', dirname)
            self.citycode = match.group(1) if match else "unknown"
        logger.info(f"ğŸ™ï¸ å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰: {self.citycode}")
        self.extracted_dir = self.data_dir / "extracted"

        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.extracted_dir.mkdir(exist_ok=True)

        # IDç®¡ç†ï¼ˆDBã‹ã‚‰æ—¢å­˜æœ€å¤§å€¤ã‚’å–å¾—ã—ã¦ç¶™ç¶šï¼‰
        self.building_id_counter = 1
        self.node_id_counter = -1  # è² ã®å€¤ã§ãƒãƒ¼ãƒ‰IDç®¡ç†

        # é‡è¤‡é™¤å»ç”¨
        self.processed_geometry_hashes = set()
        self.node_coordinate_map = {}  # åº§æ¨™ -> ãƒ¦ãƒ‹ãƒ¼ã‚¯ID ã®ãƒãƒƒãƒ”ãƒ³ã‚°

        self._test_connection()
        self._initialize_id_counters()  # DBã‹ã‚‰æ—¢å­˜IDã‚’å–å¾—

    def _initialize_id_counters(self):
        """DBã‹ã‚‰æ—¢å­˜ã®æœ€å¤§IDã‚’å–å¾—ã—ã¦ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–"""
        try:
            conn = psycopg2.connect(self.postgres_url)
            cursor = conn.cursor()

            # å»ºç‰©ã®æœ€å¤§IDã‚’å–å¾—
            cursor.execute("SELECT COALESCE(MAX(id), 0) FROM plateau_buildings")
            max_building_id = cursor.fetchone()[0]
            self.building_id_counter = max_building_id + 1

            # ãƒãƒ¼ãƒ‰ã®æœ€å°IDï¼ˆè² ã®å€¤ï¼‰ã‚’å–å¾—
            cursor.execute("SELECT COALESCE(MIN(osm_id), 0) FROM plateau_building_nodes")
            min_node_id = cursor.fetchone()[0]
            # æ—¢å­˜ã®æœ€å°å€¤ã‚ˆã‚Šã•ã‚‰ã«å°ã•ã„å€¤ã‹ã‚‰é–‹å§‹
            self.node_id_counter = min(min_node_id - 1, -1)

            # æ—¢å­˜ã®ãƒãƒ¼ãƒ‰åº§æ¨™ã‚’ãƒãƒƒãƒ—ã«èª­ã¿è¾¼ã¿ï¼ˆåŒä¸€åº§æ¨™ã¯åŒä¸€IDã‚’ä¿è¨¼ï¼‰
            cursor.execute("""
                SELECT osm_id, lat, lon 
                FROM plateau_building_nodes 
                WHERE osm_id IS NOT NULL
            """)
            existing_nodes = cursor.fetchall()
            for osm_id, lat, lon in existing_nodes:
                coord_key = f"{lat:.7f},{lon:.7f}"
                self.node_coordinate_map[coord_key] = osm_id

            conn.close()

            logger.info(f"ğŸ”¢ IDåˆæœŸåŒ–å®Œäº†:")
            logger.info(f"   å»ºç‰©IDã‚«ã‚¦ãƒ³ã‚¿ãƒ¼: {self.building_id_counter} ã‹ã‚‰é–‹å§‹")
            logger.info(f"   ãƒãƒ¼ãƒ‰IDã‚«ã‚¦ãƒ³ã‚¿ãƒ¼: {self.node_id_counter} ã‹ã‚‰é–‹å§‹")
            logger.info(f"   æ—¢å­˜ãƒãƒ¼ãƒ‰åº§æ¨™ãƒãƒƒãƒ—: {len(self.node_coordinate_map):,} ä»¶èª­ã¿è¾¼ã¿")

        except Exception as e:
            logger.warning(f"âš ï¸ IDåˆæœŸåŒ–ã§ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰: {e}")

    def _test_connection(self):
        """PostgreSQLæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            conn = psycopg2.connect(self.postgres_url)
            cursor = conn.cursor()

            # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ç¢ºèª
            cursor.execute("SELECT COUNT(*) FROM plateau_buildings")
            building_count = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM plateau_building_nodes")
            node_count = cursor.fetchone()[0]

            logger.info(f"âœ… PostgreSQLæ¥ç¶šæˆåŠŸ")
            logger.info(f"ğŸ“Š ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿: å»ºç‰©{building_count:,}ä»¶, ãƒãƒ¼ãƒ‰{node_count:,}ä»¶")

            # IDç¯„å›²ç¢ºèª
            if building_count > 0:
                cursor.execute("SELECT MIN(osm_id), MAX(osm_id) FROM plateau_buildings")
                building_range = cursor.fetchone()
                logger.info(f"ğŸ¢ å»ºç‰©IDç¯„å›²: {building_range[0]} ~ {building_range[1]}")

            if node_count > 0:
                cursor.execute("SELECT MIN(osm_id), MAX(osm_id) FROM plateau_building_nodes")
                node_range = cursor.fetchone()
                logger.info(f"ğŸ“ ãƒãƒ¼ãƒ‰IDç¯„å›²: {node_range[0]} ~ {node_range[1]}")

            conn.close()

        except Exception as e:
            logger.error(f"âŒ PostgreSQLæ¥ç¶šå¤±æ•—: {e}")
            raise

    def analyze_existing_data(self) -> Dict:
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æ"""
        logger.info("ğŸ” æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’è©³ç´°åˆ†æä¸­...")

        try:
            conn = psycopg2.connect(self.postgres_url)
            cursor = conn.cursor()

            # åŸºæœ¬çµ±è¨ˆ
            cursor.execute("SELECT COUNT(*) FROM plateau_buildings WHERE ST_IsValid(geom)")
            valid_buildings = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM plateau_buildings WHERE NOT ST_IsValid(geom)")
            invalid_buildings = cursor.fetchone()[0]

            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥çµ±è¨ˆ
            cursor.execute("""
                SELECT
                    source_dataset,
                    COUNT(*) as count
                FROM plateau_buildings
                GROUP BY source_dataset
                ORDER BY count DESC
            """)
            dataset_stats = cursor.fetchall()

            # IDåˆ©ç”¨çŠ¶æ³
            cursor.execute("SELECT MIN(osm_id), MAX(osm_id) FROM plateau_buildings WHERE osm_id > 0")
            building_id_range = cursor.fetchone()

            cursor.execute("SELECT MIN(osm_id), MAX(osm_id) FROM plateau_building_nodes WHERE osm_id < 0")
            node_id_range = cursor.fetchone()

            conn.close()

            analysis = {
                'valid_buildings': valid_buildings,
                'invalid_buildings': invalid_buildings,
                'total_buildings': valid_buildings + invalid_buildings,
                'validity_rate': valid_buildings / (valid_buildings + invalid_buildings) * 100 if (valid_buildings + invalid_buildings) > 0 else 0,
                'dataset_stats': dataset_stats,
                'building_id_range': building_id_range,
                'node_id_range': node_id_range
            }

            logger.info(f"âœ… æ—¢å­˜ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœ:")
            logger.info(f"   æœ‰åŠ¹å»ºç‰©: {analysis['valid_buildings']:,}ä»¶")
            logger.info(f"   ç„¡åŠ¹å»ºç‰©: {analysis['invalid_buildings']:,}ä»¶")
            logger.info(f"   æœ‰åŠ¹æ€§ç‡: {analysis['validity_rate']:.1f}%")
            logger.info(f"   å»ºç‰©IDç¯„å›²: {analysis['building_id_range']}")
            logger.info(f"   ãƒãƒ¼ãƒ‰IDç¯„å›²: {analysis['node_id_range']}")

            if dataset_stats:
                logger.info(f"   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥:")
                for dataset, count in dataset_stats[:5]:
                    logger.info(f"     {dataset}: {count:,}ä»¶")

            # æ¬¡ã®IDè¨­å®š
            if building_id_range and building_id_range[1]:
                self.building_id_counter = building_id_range[1] + 1
            if node_id_range and node_id_range[0]:
                self.node_id_counter = node_id_range[0] - 1

            logger.info(f"ğŸ†” æ¬¡å›ä½¿ç”¨ID: å»ºç‰©={self.building_id_counter}, ãƒãƒ¼ãƒ‰={self.node_id_counter}")

            return analysis

        except Exception as e:
            logger.error(f"âŒ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def find_zip_files(self) -> List[Path]:
        """zipãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã¨åˆ†æ"""
        logger.info(f"ğŸ“ zipãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢: {self.data_dir}")

        zip_files = list(self.data_dir.glob("*.zip"))
        zip_files.sort()

        total_size = 0
        mesh_codes = []

        for zip_file in zip_files:
            file_size = zip_file.stat().st_size
            total_size += file_size

            # ãƒ¡ãƒƒã‚·ãƒ¥ã‚³ãƒ¼ãƒ‰æŠ½å‡º
            match = re.match(r'^(\d+)', zip_file.name)
            if match:
                mesh_codes.append(match.group(1))

        logger.info(f"ğŸ“¦ ç™ºè¦‹ã—ãŸzipãƒ•ã‚¡ã‚¤ãƒ«: {len(zip_files)}ä»¶")
        logger.info(f"ğŸ“Š åˆè¨ˆã‚µã‚¤ã‚º: {total_size:,}bytes ({total_size/1024/1024:.1f}MB)")
        logger.info(f"ğŸ—‚ï¸ ãƒ¡ãƒƒã‚·ãƒ¥ã‚³ãƒ¼ãƒ‰: {len(mesh_codes)}ä»¶")

        if mesh_codes:
            logger.info(f"   ãƒ¡ãƒƒã‚·ãƒ¥ä¾‹: {', '.join(sorted(mesh_codes)[:10])}")
            if len(mesh_codes) > 10:
                logger.info(f"   ... (ä»– {len(mesh_codes)-10}ä»¶)")

        return zip_files

    def extract_zip_files(self, zip_files: List[Path]) -> List[Path]:
        """zipãƒ•ã‚¡ã‚¤ãƒ«å±•é–‹ï¼ˆé‡è¤‡å›é¿ï¼‰"""
        logger.info(f"ğŸ“‚ {len(zip_files)}ä»¶ã®zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹ä¸­...")

        osm_files = []
        processed_count = 0

        for i, zip_path in enumerate(zip_files, 1):
            try:
                # å±•é–‹å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                extract_subdir = self.extracted_dir / zip_path.stem
                extract_subdir.mkdir(exist_ok=True)

                # æ—¢ã«å±•é–‹æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
                existing_osm = list(extract_subdir.glob("*.osm"))
                if existing_osm:
                    logger.info(f"â­ï¸ [{i:3d}/{len(zip_files)}] ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å­˜ï¼‰: {zip_path.name}")
                    osm_files.extend(existing_osm)
                    continue

                logger.info(f"ğŸ“‚ [{i:3d}/{len(zip_files)}] å±•é–‹ä¸­: {zip_path.name}")

                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ç¢ºèª
                    file_list = zip_ref.namelist()
                    osm_count = len([f for f in file_list if f.endswith('.osm')])

                    if osm_count == 0:
                        logger.warning(f"     âš ï¸ OSMãƒ•ã‚¡ã‚¤ãƒ«ãªã—")
                        continue

                    # å±•é–‹å®Ÿè¡Œ
                    zip_ref.extractall(extract_subdir)
                    processed_count += 1

                # OSMãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
                for osm_path in extract_subdir.rglob("*.osm"):
                    osm_files.append(osm_path)
                    logger.info(f"     ğŸ“„ OSMç™ºè¦‹: {osm_path.name}")

            except zipfile.BadZipFile:
                logger.warning(f"âŒ ä¸æ­£ãªzipãƒ•ã‚¡ã‚¤ãƒ«: {zip_path.name}")
                continue
            except Exception as e:
                logger.warning(f"âŒ å±•é–‹å¤±æ•—: {zip_path.name} - {e}")
                continue

        logger.info(f"âœ… å±•é–‹å®Œäº†: {processed_count}ä»¶å‡¦ç†, {len(osm_files)}å€‹ã®OSMãƒ•ã‚¡ã‚¤ãƒ«")
        return osm_files

    def parse_osm_file_safe(self, osm_file: Path) -> Tuple[Dict, List]:
        """å®‰å…¨ãªOSMãƒ•ã‚¡ã‚¤ãƒ«è§£æï¼ˆä¿®å¾©æ¸ˆã¿æŠ€è¡“ï¼‰"""
        try:
            tree = ET.parse(osm_file)
            root = tree.getroot()
        except ET.ParseError as e:
            logger.warning(f"âŒ XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ {osm_file.name}: {e}")
            return {}, []

        file_prefix = osm_file.stem
        nodes = {}
        buildings = []

        # ãƒãƒ¼ãƒ‰åé›†ï¼ˆåº§æ¨™æ¤œè¨¼ä»˜ãï¼‰
        for node_elem in root.findall('node'):
            original_id = node_elem.get('id')
            try:
                lat = float(node_elem.get('lat'))
                lon = float(node_elem.get('lon'))

                # åº§æ¨™ç¯„å›²ãƒã‚§ãƒƒã‚¯ï¼ˆæŒ‡å®šãŒãªã‘ã‚Œã°æ—¥æœ¬å…¨åŸŸï¼‰
                if self.coord_bounds:
                    min_lat, max_lat, min_lon, max_lon = self.coord_bounds
                    in_bounds = min_lat <= lat <= max_lat and min_lon <= lon <= max_lon
                else:
                    in_bounds = 20.0 <= lat <= 46.0 and 122.0 <= lon <= 154.0
                if in_bounds:
                    # åº§æ¨™ãƒ™ãƒ¼ã‚¹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯IDç”Ÿæˆï¼ˆä¿®å¾©æ¸ˆã¿æŠ€è¡“ï¼‰
                    coord_key = f"{lat:.7f},{lon:.7f}"

                    if coord_key in self.node_coordinate_map:
                        # æ—¢å­˜åº§æ¨™ã®å ´åˆã¯æ—¢å­˜IDã‚’ä½¿ç”¨
                        unique_id = self.node_coordinate_map[coord_key]
                    else:
                        # æ–°åº§æ¨™ã®å ´åˆã¯æ–°IDã‚’å‰²ã‚Šå½“ã¦
                        unique_id = self.node_id_counter
                        self.node_coordinate_map[coord_key] = unique_id
                        self.node_id_counter -= 1

                    nodes[original_id] = {
                        'unique_id': unique_id,
                        'lat': lat,
                        'lon': lon,
                        'coord_key': coord_key
                    }
            except (ValueError, TypeError):
                continue

        # å»ºç‰©ã‚¦ã‚§ã‚¤åé›†
        for way_elem in root.findall('way'):
            tags = {}
            for tag_elem in way_elem.findall('tag'):
                key = tag_elem.get('k')
                value = tag_elem.get('v')
                if key and value:
                    tags[key] = value

            # å»ºç‰©åˆ¤å®š
            if tags.get('building'):
                way_id = way_elem.get('id')
                nd_refs = []

                for nd_elem in way_elem.findall('nd'):
                    nd_ref = nd_elem.get('ref')
                    if nd_ref in nodes:
                        nd_refs.append(nd_ref)

                # æœ€ä½3ç‚¹ã§ãƒãƒªã‚´ãƒ³å½¢æˆ
                if len(nd_refs) >= 3:
                    buildings.append({
                        'way_id': way_id,
                        'tags': tags,
                        'node_refs': nd_refs,
                        'source_file': osm_file.name,
                        'file_prefix': file_prefix
                    })

        return nodes, buildings

    def convert_building_tags_enhanced(self, tags: Dict, source_info: str) -> Dict:
        """å»ºç‰©ã‚¿ã‚°å¤‰æ›ï¼ˆå“è³ªå‘ä¸Šç‰ˆï¼‰"""
        result = {
            'building': 'yes',
            'height': None,
            'ele': None,
            'building_levels': None,
            'name': None,
            'addr_housenumber': None,
            'addr_street': None,
            'building_material': None,
            'roof_material': None,
            'roof_shape': None,
            'start_date': None,
            'amenity': None,
            'shop': None,
            'tourism': None,
            'leisure': None,
            'landuse': None,
            'source_dataset': f"plateau_{self.citycode}_{source_info}"
        }

        # åŸºæœ¬å»ºç‰©ã‚¿ã‚¤ãƒ—
        building_type = tags.get('building', 'yes')
        if building_type and building_type != 'no':
            result['building'] = building_type

        # é«˜ã•æƒ…å ±ï¼ˆå³æ ¼æ¤œè¨¼ï¼‰
        height_raw = tags.get('height')
        if height_raw:
            try:
                height_val = float(height_raw)
                if 0.5 <= height_val <= 300:  # ç¾å®Ÿçš„ãªå»ºç‰©é«˜ã•
                    result['height'] = height_val
            except (ValueError, TypeError):
                pass

        # éšæ•°
        levels_raw = tags.get('building:levels')
        if levels_raw:
            try:
                levels = int(float(levels_raw))
                if 1 <= levels <= 50:
                    result['building_levels'] = levels
            except (ValueError, TypeError):
                pass

        # å»ºç‰©åç§°
        name = tags.get('name') or tags.get('name:ja')
        if name:
            result['name'] = name[:100]

        # ä½æ‰€æƒ…å ±
        addr_housenumber = tags.get('addr:housenumber')
        if addr_housenumber:
            result['addr_housenumber'] = addr_housenumber[:20]

        addr_street = tags.get('addr:street')
        if addr_street:
            result['addr_street'] = addr_street[:100]

        # å»ºæãƒ»å±‹æ ¹æƒ…å ±
        building_material = tags.get('building:material')
        if building_material:
            result['building_material'] = building_material[:50]

        roof_material = tags.get('roof:material')
        if roof_material:
            result['roof_material'] = roof_material[:50]

        roof_shape = tags.get('roof:shape')
        if roof_shape:
            result['roof_shape'] = roof_shape[:50]

        # æ¨™é«˜æƒ…å ±
        ele_raw = tags.get('ele')
        if ele_raw:
            try:
                ele_val = float(ele_raw)
                if -100 <= ele_val <= 9000:  # ç¾å®Ÿçš„ãªæ¨™é«˜ç¯„å›²
                    result['ele'] = ele_val
            except (ValueError, TypeError):
                pass

        # å»ºè¨­å¹´
        start_date = tags.get('start_date')
        if start_date:
            result['start_date'] = start_date[:10]

        # ç”¨é€”ãƒ»æ–½è¨­æƒ…å ±
        for key in ['amenity', 'shop', 'tourism', 'leisure', 'landuse']:
            value = tags.get(key)
            if value:
                result[key] = value[:50]

        return result

    def create_geometry_hash(self, coords: List[Tuple[float, float]]) -> str:
        """ã‚¸ã‚ªãƒ¡ãƒˆãƒªãƒãƒƒã‚·ãƒ¥ç”Ÿæˆï¼ˆé‡è¤‡é™¤å»ç”¨ï¼‰"""
        # åº§æ¨™ã‚’æ­£è¦åŒ–ã—ã¦ãƒãƒƒã‚·ãƒ¥åŒ–
        normalized_coords = []
        for lon, lat in coords:
            normalized_coords.append((round(lon, 7), round(lat, 7)))

        # ã‚½ãƒ¼ãƒˆã—ã¦å‘ãçµ±ä¸€
        normalized_coords.sort()
        coord_str = '|'.join([f"{lon},{lat}" for lon, lat in normalized_coords])

        return hashlib.md5(coord_str.encode()).hexdigest()

    def process_buildings_safe(self, all_nodes: Dict, all_buildings: List) -> Tuple[List, List]:
        """å»ºç‰©å‡¦ç†ï¼ˆå®‰å…¨ç‰ˆãƒ»é‡è¤‡é™¤å»ä»˜ãï¼‰"""
        logger.info(f"ğŸ—ï¸ {len(all_buildings):,}å»ºç‰©ã‚’å®‰å…¨å‡¦ç†ä¸­...")

        buildings_data = []
        nodes_data = []
        processed_count = 0
        skipped_count = 0
        duplicate_count = 0

        for i, building in enumerate(all_buildings, 1):
            try:
                # é€²æ—è¡¨ç¤º
                if i % 1000 == 0:
                    progress = (i / len(all_buildings)) * 100
                    logger.info(f"ğŸ”„ å‡¦ç†ä¸­: {i:,}/{len(all_buildings):,} ({progress:.1f}%) - æˆåŠŸ:{processed_count}, é‡è¤‡:{duplicate_count}, ã‚¹ã‚­ãƒƒãƒ—:{skipped_count}")

                tags = building['tags']
                node_refs = building['node_refs']
                source_file = building['source_file']

                # åº§æ¨™åé›†ãƒ»ãƒ¦ãƒ‹ãƒ¼ã‚¯IDä½¿ç”¨
                coords = []
                building_nodes = []

                for seq, original_node_ref in enumerate(node_refs):
                    if original_node_ref in all_nodes:
                        node_data = all_nodes[original_node_ref]
                        unique_node_id = node_data['unique_id']
                        lat = node_data['lat']
                        lon = node_data['lon']

                        coords.append((lon, lat))

                        # ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯IDä½¿ç”¨ï¼‰
                        building_nodes.append((
                            unique_node_id,        # idï¼ˆè² ã®å€¤ï¼‰
                            self.building_id_counter,  # building_id
                            seq,                   # sequence_id
                            lat,                   # lat
                            lon,                   # lon
                            lon,                   # ST_Pointç”¨ lon
                            lat                    # ST_Pointç”¨ lat
                        ))

                # ãƒãƒªã‚´ãƒ³å½¢æˆãƒã‚§ãƒƒã‚¯
                if len(coords) >= 3:
                    # ãƒãƒªã‚´ãƒ³é–‰é–
                    if coords[0] != coords[-1]:
                        coords.append(coords[0])

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    geom_hash = self.create_geometry_hash(coords[:-1])  # é–‰é–ç‚¹é™¤å¤–ã§ãƒãƒƒã‚·ãƒ¥

                    if geom_hash in self.processed_geometry_hashes:
                        duplicate_count += 1
                        continue

                    self.processed_geometry_hashes.add(geom_hash)

                    # é¢ç©ãƒã‚§ãƒƒã‚¯ï¼ˆæ¥µå°ãƒãƒªã‚´ãƒ³é™¤å¤–ï¼‰
                    if len(coords) >= 4:
                        # ç°¡æ˜“é¢ç©è¨ˆç®—
                        area_check = True
                        if len(coords) == 4:  # ä¸‰è§’å½¢
                            x1, y1 = coords[0]
                            x2, y2 = coords[1]
                            x3, y3 = coords[2]
                            area = abs((x1*(y2-y3) + x2*(y3-y1) + x3*(y1-y2))/2)
                            if area < 0.000001:  # æ¥µå°é¢ç©
                                area_check = False

                        if area_check:
                            # ã‚¿ã‚°å¤‰æ›
                            converted_tags = self.convert_building_tags_enhanced(tags, source_file)

                            # WKTä½œæˆ
                            coords_str = ','.join([f"{lon} {lat}" for lon, lat in coords])
                            polygon_wkt = f"POLYGON(({coords_str}))"

                            # ä½æ‰€ã‚’çµåˆ
                            addr_parts = []
                            if converted_tags.get('addr_street'):
                                addr_parts.append(converted_tags['addr_street'])
                            if converted_tags.get('addr_housenumber'):
                                addr_parts.append(converted_tags['addr_housenumber'])
                            addr_full = ' '.join(addr_parts) if addr_parts else None

                            # å»ºç‰©ãƒ‡ãƒ¼ã‚¿ï¼ˆplateau_buildingsãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã«åˆã‚ã›ã‚‹ï¼‰
                            buildings_data.append((
                                self.building_id_counter,           # osm_id
                                converted_tags.get('building', 'yes'),  # building
                                converted_tags.get('height'),       # height
                                converted_tags.get('ele'),          # ele
                                converted_tags.get('building_levels'),  # building_levels
                                None,                               # building_levels_underground
                                converted_tags.get('source_dataset'),   # source_dataset
                                building['way_id'],                 # plateau_id
                                polygon_wkt,                        # geometry_wkt
                                converted_tags.get('name'),         # name
                                addr_full,                          # addr_full
                                converted_tags.get('addr_housenumber'), # addr_housenumber
                                converted_tags.get('addr_street'),  # addr_street
                                converted_tags.get('start_date'),   # start_date
                                converted_tags.get('building_material'), # building_material
                                converted_tags.get('roof_material'),    # roof_material
                                converted_tags.get('roof_shape'),       # roof_shape
                                converted_tags.get('amenity'),      # amenity
                                converted_tags.get('shop'),         # shop
                                converted_tags.get('tourism'),      # tourism
                                converted_tags.get('leisure'),      # leisure
                                converted_tags.get('landuse'),      # landuse
                                polygon_wkt,                        # geomç”¨WKT
                                polygon_wkt                         # centroidç”¨WKT
                            ))

                            nodes_data.extend(building_nodes)
                            self.building_id_counter += 1
                            processed_count += 1
                        else:
                            skipped_count += 1
                    else:
                        skipped_count += 1
                else:
                    skipped_count += 1

            except Exception as e:
                logger.warning(f"âš ï¸ å»ºç‰©å‡¦ç†ã‚¨ãƒ©ãƒ¼ {i}: {e}")
                skipped_count += 1
                continue

        logger.info(f"ğŸ“Š å»ºç‰©å‡¦ç†çµæœ:")
        logger.info(f"   æˆåŠŸ: {processed_count:,}ä»¶")
        logger.info(f"   é‡è¤‡é™¤å»: {duplicate_count:,}ä»¶")
        logger.info(f"   ã‚¹ã‚­ãƒƒãƒ—: {skipped_count:,}ä»¶")
        logger.info(f"   ç·ãƒãƒ¼ãƒ‰: {len(nodes_data):,}ä»¶")

        return buildings_data, nodes_data

    def insert_to_database_safe(self, buildings_data: List, nodes_data: List) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å®‰å…¨æŠ•å…¥ï¼ˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†ãƒ»é‡è¤‡å›é¿ï¼‰"""
        logger.info(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«å®‰å…¨æŠ•å…¥ä¸­...")
        logger.info(f"   å»ºç‰©: {len(buildings_data):,}ä»¶")
        logger.info(f"   ãƒãƒ¼ãƒ‰: {len(nodes_data):,}ä»¶")

        conn = psycopg2.connect(self.postgres_url)

        try:
            cursor = conn.cursor()

            # å»ºç‰©æŠ•å…¥
            # ä¸å®Œå…¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å…ˆã«å‰Šé™¤ï¼ˆcitycodeæŒ‡å®šæ™‚ï¼‰
            if self.citycode and self.citycode != "unknown":
                cursor.execute(
                    "SELECT COUNT(*) FROM plateau_buildings WHERE source_dataset LIKE %s",
                    (f"%{self.citycode}%",)
                )
                existing_count = cursor.fetchone()[0]
                if existing_count > 0:
                    logger.info(f"ğŸ§¹ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿æ¤œå‡º: {self.citycode} ({existing_count}ä»¶) â€” å‰Šé™¤ã—ã¦å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
                    # ãƒãƒ¼ãƒ‰ã‚’å…ˆã«å‰Šé™¤ï¼ˆforeign keyåˆ¶ç´„ï¼‰
                    cursor.execute("""
                        DELETE FROM plateau_building_nodes
                        WHERE building_id IN (
                            SELECT id FROM plateau_buildings WHERE source_dataset LIKE %s
                        )
                    """, (f"%{self.citycode}%",))
                    cursor.execute(
                        "DELETE FROM plateau_buildings WHERE source_dataset LIKE %s",
                        (f"%{self.citycode}%",)
                    )
                    conn.commit()
                    logger.info(f"âœ… æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤å®Œäº†")

            if buildings_data:
                logger.info("ğŸ¢ å»ºç‰©ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ä¸­...")

                execute_values(
                    cursor,
                    """
                    INSERT INTO plateau_buildings
                    (osm_id, building, height, ele, building_levels, building_levels_underground,
                     source_dataset, plateau_id, geometry_wkt,
                     name, addr_full, addr_housenumber, addr_street,
                     start_date, building_material, roof_material, roof_shape,
                     amenity, shop, tourism, leisure, landuse,
                     geom, centroid)
                    VALUES %s
                    """,
                    buildings_data,
                    template="(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, ST_GeomFromText(%s, 4326), ST_Centroid(ST_GeomFromText(%s, 4326)))",
                    page_size=1000
                )
                logger.info("âœ… å»ºç‰©æŠ•å…¥å®Œäº†")

            # ãƒãƒ¼ãƒ‰æŠ•å…¥
            if nodes_data:
                logger.info("ğŸ“ ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ä¸­...")

                # ä»Šå›æŠ•å…¥ã™ã‚‹building_idã®é›†åˆã‚’å–å¾—ï¼ˆç›´å‰ã«INSERTã—ãŸå»ºç‰©ã®ã¿ï¼‰
                cursor.execute(
                    "SELECT id FROM plateau_buildings WHERE source_dataset LIKE %s",
                    (f"%{self.citycode}%",)
                )
                current_building_ids = set(row[0] for row in cursor.fetchall())
                logger.info(f"   ä»Šå›ã®å»ºç‰©ID: {len(current_building_ids):,}ä»¶")

                # ä»Šå›ã®å»ºç‰©ã«å±ã™ã‚‹ãƒãƒ¼ãƒ‰ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ & ãƒ‡ãƒ¼ã‚¿å†…é‡è¤‡é™¤å»
                unique_nodes_data = []
                seen_node_ids = set()
                skipped_count = 0
                orphan_count = 0

                for node_data in nodes_data:
                    node_id = node_data[0]  # osm_id
                    building_id = node_data[1]  # building_id
                    if node_id in seen_node_ids:
                        skipped_count += 1
                    elif building_id not in current_building_ids:
                        orphan_count += 1
                    else:
                        unique_nodes_data.append(node_data)
                        seen_node_ids.add(node_id)

                if orphan_count > 0:
                    logger.warning(f"   âš ï¸ å»ºç‰©ãªã—ãƒãƒ¼ãƒ‰é™¤å¤–: {orphan_count:,}ä»¶")

                logger.info(f"   æŠ•å…¥ãƒãƒ¼ãƒ‰: {len(unique_nodes_data):,}ä»¶")
                logger.info(f"   é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {skipped_count:,}ä»¶")

                if unique_nodes_data:
                    execute_values(
                        cursor,
                        """
                        INSERT INTO plateau_building_nodes (osm_id, building_id, sequence_id, lat, lon, geom)
                        VALUES %s
                        """,
                        unique_nodes_data,
                        template="(%s, %s, %s, %s, %s, ST_Point(%s, %s))",
                        page_size=5000
                    )
                logger.info("âœ… ãƒãƒ¼ãƒ‰æŠ•å…¥å®Œäº†")

            # ã‚³ãƒŸãƒƒãƒˆ
            conn.commit()

            # æœ€çµ‚ç¢ºèª
            cursor.execute("SELECT COUNT(*) FROM plateau_buildings")
            final_buildings = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM plateau_building_nodes")
            final_nodes = cursor.fetchone()[0]

            # æœ‰åŠ¹æ€§ç¢ºèª
            cursor.execute("SELECT COUNT(*) FROM plateau_buildings WHERE ST_IsValid(geom)")
            valid_buildings = cursor.fetchone()[0]

            validity_rate = valid_buildings / final_buildings * 100 if final_buildings > 0 else 0

            logger.info(f"ğŸ‰ æŠ•å…¥å®Œäº†!")
            logger.info(f"ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿: å»ºç‰©{final_buildings:,}ä»¶, ãƒãƒ¼ãƒ‰{final_nodes:,}ä»¶")
            logger.info(f"âœ… ã‚¸ã‚ªãƒ¡ãƒˆãƒªæœ‰åŠ¹æ€§: {validity_rate:.1f}% ({valid_buildings:,}/{final_buildings:,})")

            return True

        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŠ•å…¥å¤±æ•—: {e}")
            conn.rollback()
            raise
        finally:
            conn.close()

    def create_import_report(self, start_analysis: Dict, zip_count: int, osm_count: int,
                           building_processed: int, node_processed: int):
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ"""
        report_file = self.data_dir / "import_report.txt"

        # æœ€çµ‚åˆ†æ
        final_analysis = self.analyze_existing_data()

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# Plateauå»ºç‰©ãƒ‡ãƒ¼ã‚¿ ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ¬ãƒãƒ¼ãƒˆ\n")
            f.write(f"# å®Ÿè¡Œæ—¥æ™‚: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            f.write("## ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‰çŠ¶æ³\n")
            f.write(f"å»ºç‰©æ•°: {start_analysis.get('total_buildings', 0):,}ä»¶\n")
            f.write(f"æœ‰åŠ¹æ€§ç‡: {start_analysis.get('validity_rate', 0):.1f}%\n")

            f.write("\n## å‡¦ç†ãƒ‡ãƒ¼ã‚¿\n")
            f.write(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«: {zip_count}ä»¶\n")
            f.write(f"OSMãƒ•ã‚¡ã‚¤ãƒ«: {osm_count}ä»¶\n")
            f.write(f"æ–°è¦å»ºç‰©: {building_processed:,}ä»¶\n")
            f.write(f"æ–°è¦ãƒãƒ¼ãƒ‰: {node_processed:,}ä»¶\n")

            f.write("\n## ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¾ŒçŠ¶æ³\n")
            f.write(f"ç·å»ºç‰©æ•°: {final_analysis.get('total_buildings', 0):,}ä»¶\n")
            f.write(f"æœ‰åŠ¹æ€§ç‡: {final_analysis.get('validity_rate', 0):.1f}%\n")

            building_increase = final_analysis.get('total_buildings', 0) - start_analysis.get('total_buildings', 0)
            f.write(f"å»ºç‰©å¢—åŠ : +{building_increase:,}ä»¶\n")

            if final_analysis.get('validity_rate', 0) >= 99.9:
                f.write("\nâœ… é«˜å“è³ªã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\n")

        logger.info(f"ğŸ“‹ ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ: {report_file}")

    def run_complete_import(self):
        """å®Œå…¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè¡Œ"""
        logger.info("ğŸš€ Plateauå»ºç‰©ãƒ‡ãƒ¼ã‚¿ PostGISã‚¤ãƒ³ãƒãƒ¼ãƒˆé–‹å§‹")
        logger.info("=" * 60)

        start_time = time.time()

        try:
            # Phase 1: äº‹å‰åˆ†æ
            logger.info("\nğŸ“Š Phase 1: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿åˆ†æ")
            start_analysis = self.analyze_existing_data()

            # Phase 2: zipãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
            logger.info("\nğŸ“ Phase 2: zipãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª")
            zip_files = self.find_zip_files()
            if not zip_files:
                logger.error("âŒ zipãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                logger.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„")
                return False

            # Phase 3: OSMæŠ½å‡º
            logger.info("\nğŸ“‚ Phase 3: OSMå±•é–‹ãƒ»æŠ½å‡º")
            osm_files = self.extract_zip_files(zip_files)
            if not osm_files:
                logger.error("âŒ OSMãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False

            # Phase 4: OSMè§£æ
            logger.info("\nğŸ“– Phase 4: OSMè§£æãƒ»çµ±åˆ")
            all_nodes = {}
            all_buildings = []

            for i, osm_file in enumerate(osm_files, 1):
                logger.info(f"ğŸ“– [{i:3d}/{len(osm_files)}] è§£æä¸­: {osm_file.name}")

                nodes, buildings = self.parse_osm_file_safe(osm_file)

                # ãƒãƒ¼ãƒ‰çµ±åˆï¼ˆé‡è¤‡åº§æ¨™ã¯åŒä¸€IDã«ï¼‰
                for original_id, node_data in nodes.items():
                    file_specific_key = f"{osm_file.name}:{original_id}"
                    all_nodes[file_specific_key] = node_data

                # å»ºç‰©çµ±åˆ
                for building in buildings:
                    # ãƒãƒ¼ãƒ‰å‚ç…§ã‚’ãƒ•ã‚¡ã‚¤ãƒ«å›ºæœ‰ã‚­ãƒ¼ã«å¤‰æ›´
                    building['node_refs'] = [f"{osm_file.name}:{ref}" for ref in building['node_refs']]
                    all_buildings.append(building)

                logger.info(f"     çµæœ: {len(nodes):,}ãƒãƒ¼ãƒ‰, {len(buildings):,}å»ºç‰©")

            logger.info(f"ğŸ“Š çµ±åˆçµæœ: {len(all_nodes):,}ãƒãƒ¼ãƒ‰, {len(all_buildings):,}å»ºç‰©")
            logger.info(f"ğŸ†” ãƒ¦ãƒ‹ãƒ¼ã‚¯åº§æ¨™: {len(self.node_coordinate_map):,}ç®‡æ‰€")

            # Phase 5: å»ºç‰©å‡¦ç†
            logger.info("\nğŸ—ï¸ Phase 5: å»ºç‰©ãƒ‡ãƒ¼ã‚¿å‡¦ç†")
            buildings_data, nodes_data = self.process_buildings_safe(all_nodes, all_buildings)

            if not buildings_data:
                logger.error("âŒ å‡¦ç†å¯èƒ½ãªå»ºç‰©ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return False

            # Phase 6: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŠ•å…¥
            logger.info("\nğŸ’¾ Phase 6: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŠ•å…¥")
            success = self.insert_to_database_safe(buildings_data, nodes_data)

            if not success:
                return False

            # Phase 7: ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
            logger.info("\nğŸ“‹ Phase 7: ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ")
            self.create_import_report(
                start_analysis, len(zip_files), len(osm_files),
                len(buildings_data), len(nodes_data)
            )

            # å®Œäº†æ™‚é–“
            elapsed_time = time.time() - start_time

            logger.info("=" * 60)
            logger.info("ğŸ‰ Plateauå»ºç‰©ãƒ‡ãƒ¼ã‚¿ PostGISã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ!")
            logger.info(f"â±ï¸ å‡¦ç†æ™‚é–“: {elapsed_time/60:.1f}åˆ†")
            logger.info(f"ğŸ¢ æ–°è¦å»ºç‰©: {len(buildings_data):,}ä»¶")
            logger.info(f"ğŸ“ æ–°è¦ãƒãƒ¼ãƒ‰: {len(nodes_data):,}ä»¶")
            logger.info("âœ… æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
            logger.info("   1. APIå‹•ä½œç¢ºèª")
            logger.info("   2. RapiD Editorè¡¨ç¤ºãƒ†ã‚¹ãƒˆ")
            logger.info("   3. ã‚«ãƒãƒ¬ãƒƒã‚¸æ¤œè¨¼")
            logger.info("=" * 60)

            return True

        except Exception as e:
            logger.error(f"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    import argparse

    parser = argparse.ArgumentParser(description='Plateauå»ºç‰©ãƒ‡ãƒ¼ã‚¿ PostGISã‚¤ãƒ³ãƒãƒ¼ã‚¿ãƒ¼')
    parser.add_argument('--data-dir', default='./plateau_data',
                       help='ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (default: ./plateau_data)')
    parser.add_argument('--postgres-url',
                       default='postgresql://osmfj_user:secure_plateau_password@localhost:5432/osmfj_plateau',
                       help='PostgreSQLæ¥ç¶šURL')
    parser.add_argument('--citycode',
                       help='å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰ (ä¾‹: "31202")ã€‚æœªæŒ‡å®šæ™‚ã¯data-dirã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰æ¨å®š')
    parser.add_argument('--coord-bounds',
                       help='åº§æ¨™ç¯„å›²ãƒã‚§ãƒƒã‚¯: "min_lat,max_lat,min_lon,max_lon" (ä¾‹: "35.2,35.6,133.0,133.5")')
    parser.add_argument('--verbose', action='store_true',
                       help='è©³ç´°ãƒ­ã‚°å‡ºåŠ›')

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    coord_bounds = None
    if args.coord_bounds:
        coord_bounds = tuple(float(x) for x in args.coord_bounds.split(','))

    logger.info("ğŸ—ï¸ Plateauå»ºç‰©ãƒ‡ãƒ¼ã‚¿ PostGISã‚¤ãƒ³ãƒãƒ¼ã‚¿ãƒ¼èµ·å‹•")

    importer = PlateauImporter2PostGIS(args.data_dir, args.postgres_url, coord_bounds, args.citycode)
    success = importer.run_complete_import()

    if success:
        logger.info("âœ… ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸï¼APIãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        print("\nğŸ‰ ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ!")
        print("ğŸš€ æ¬¡ã¯ APIå‹•ä½œç¢ºèªã¨RapiD Editorãƒ†ã‚¹ãƒˆ")
    else:
        logger.error("âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        print("\nâŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        print("ğŸ“‹ è©³ç´°: plateau_importer2postgis.log ã‚’ç¢ºèª")
        sys.exit(1)

if __name__ == "__main__":
    main()
